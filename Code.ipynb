!pip install scikit-optimize #if skopt doesnt get imported

# import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sn
from scipy.stats import zscore
from sklearn.ensemble import RandomForestRegressor as rfr
from sklearn.model_selection import cross_val_score as cvs
import sklearn.metrics as metric
from skopt import BayesSearchCV as bs

# Step - 1 : Problem Statement : To predict Infinite Multiplication Factor(IMF) and PPPF

# Step - 2 : Data Acquisition 
data = pd.read_fwf("/content/raw.csv", header = None)
test = pd.read_fwf("/content/test.csv",header = None)
data.isnull().sum().to_numpy() # shows that the dataset has no missing values

#Step - 3 : Data Exploration
data.describe()
# from the mean,std,min,max of each column it is clear that column 0 is pppf and column 1 is IMF

data.rename(columns = {0:"PPPF",1:"IMF"},inplace = True)
for i in range(2,41):
    plt.boxplot(x = data[i] )
    plt.show();
#below are the box plots of all the 39 featurs and none of them have potential outliers

box_imf = plt.boxplot(x = data["IMF"])
plt.title("IMF")
plt.figure()
box_pppf = plt.boxplot(x = data["PPPF"])
plt.title("PPPF")
# the box plot shows that there are potential outliers in IMF and PPPF which need to be removed for a better dataset

# the outliers can be found calculating the Z - Score
data["z_score_imf"] = zscore(data["IMF"])
data["z_score_pppf"] = zscore(data["PPPF"])
data.drop(data[(data["z_score_imf"]>3) | (data["z_score_imf"]<-3)|(data["z_score_pppf"]>3)|(data["z_score_imf"]<-3)].index,axis = 0,inplace = True)
data.drop(data[["z_score_imf","z_score_pppf"]],axis = 1,inplace = True)
# the potential outliers are removed

i = data.corr()
for r in range(2,41):
    print(i[((i[r]>0.5)|(i[r]<-0.5))&(i[r]!=1)])
# the above code tells us that none of the features are highly correlated(r>0.5) and so none of the features have to be removed from the dataset 

sn.heatmap(i) # the heatmap also suggests the same idea that none of the 39 features are highly correlated

for i in range(2,40):
    sn.regplot(x = data.iloc[:,i], y = data.iloc[:, 0])
    plt.show()
# the scatter plot shows that the 39 features are randomly scattered wrt PPPF

for i in range(2,40):
    sn.regplot(x = data.iloc[:,i], y = data.iloc[:,1])
    plt.show()
# the scatter plot shows that the 39 features are randomly scattered wrt IMF

# Step - 4 : Modelling
# as the features are randomly scattered wrt the target variables(IMF and PPPF) and we also have thousands of observations, we should use a tree based model,further as there is low correlation between targets and features we use Random forest Regression  
X_train = data[range(2,41)]
Y_train_pppf = data["PPPF"]
Y_train_imf = data["IMF"]
X_test = test[range(2,41)]
Y_test_pppf = test[0]
Y_test_imf = test[1]
model_pppf = rfr().fit(X_train, Y_train_pppf)
model_imf = rfr().fit(X_train, Y_train_imf)

# Step - 5 : Evaluation
print(f"The r2 score for the model predicting PPPF : {metric.r2_score(Y_test_pppf,model_pppf.predict(X_test))}\nThe RMSE : {np.sqrt(metric.mean_squared_error(Y_test_pppf,model_pppf.predict(X_test)))}\nThe r2 score for the model predicting IMF : {metric.r2_score(Y_test_imf,model_imf.predict(X_test))}\nThe RMSE : {np.sqrt(metric.mean_squared_error(Y_test_imf,model_imf.predict(X_test)))}")

# we do hyperparameter tuning and cross validation
params = {"n_estimators" : (50,100), "max_depth" : (5,30), "max_features" : ("sqrt","log2")}       
bs_pppf = bs(estimator = rfr(),search_spaces=params,scoring='r2', cv = 5).fit(X_train, Y_train_pppf)
bs_imf = bs(estimator = rfr(),search_spaces=params,scoring='r2', cv = 5).fit(X_train, Y_train_imf)
bs_imf.best_score_
